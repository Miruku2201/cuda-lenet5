{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><b>CSC14120 – PARALLEL PROGRAMMING</b></h1>\n",
    "\n",
    "<h2 align=center>FINAL PROJECT</h2>\n",
    "<p style=\"font-size:32px;text-align:center\">Parallelize Convolutional Layer in the LeNet-5 Architecture using CUDA</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student information**\n",
    "\n",
    "Student name         | Student ID \n",
    "---------------------|-------------\n",
    "Nguyễn Quang Gia Bảo | 20120040\n",
    "Huỳnh Tuấn Nam       | 20120136\n",
    "Trần Hoàng Anh Phi   | 20120158"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Table of Contents__\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "   1. [Objectives](#objectives)\n",
    "   2. [Dataset](#dataset)\n",
    "   3. [Motivation](#motivation)\n",
    "2. [Background](#background)\n",
    "   1. [Convolutional Neural Networks (CNNs)](#cnns)\n",
    "   2. [LeNet-5](#lenet-5)\n",
    "3. [Starter project](#starter)\n",
    "4. [Implementation](#implementation)\n",
    "   1. [Convolutional Layer](#convolutional-layer)\n",
    "   2. [Pooling Layer](#pooling-layer)\n",
    "   3. [Fully Connected Layer](#fully-connected-layer)\n",
    "   4. [LeNet-5 Architecture](#lenet-5-architecture)\n",
    "   5. [Parallelization](#parallelization)\n",
    "      1. [Basic kernel](#basickernel)\n",
    "      2. [Optimized kernel 1](#kernel1)\n",
    "      3. [Optimized kernel 2](#kernel2)\n",
    "      4. [Optimized kernel 3](#kernel3)\n",
    "5. [Usage](#usage)\n",
    "   1. [Prerequisites](#prerequisites)\n",
    "   2. [Training](#training)\n",
    "   3. [Testing](#testing)\n",
    "6. [Results](#results)\n",
    "   1. [Training stage](#training-stage)\n",
    "   2. [Testing stage](#testing-stage)\n",
    "      1. [CPU](#cpu) \n",
    "      2. [Basic kernel](#basic-kernel)\n",
    "      3. [Optimized kernel 1](#optimized-kernel-1)\n",
    "      4. [Optimized kernel 2](#optimized-kernel-2)\n",
    "      5. [Optimized kernel 3](#optimized-kernel-3)\n",
    "7. [Conclusion](#conclusion)\n",
    "8. [Reflection](#reflection)\n",
    "9. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Workload distribution:__\n",
    "\n",
    "Name | Student ID | Contribution | Percentage\n",
    "-----|------------|--------------|------------\n",
    "Nguyễn Quang Gia Bảo | 20120040 | Planning, read documents, implement forward-pass for CPU | 100%\n",
    "Huỳnh Tuấn Nam | 20120136 | Planning, read documents, implement training phase, write report | 100%\n",
    "Trần Hoàng Anh Phi | 20120158 | Implement training phase, basic kernel, optimized kernel 1, optimized kernel 2, optimized kernel 3, write report | 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1    Introduction <a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1    Objectives <a id=\"objectives\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objective__: Implement and optimize the forward-pass of a convolutional layer using CUDA. \n",
    "- Convolutional layers are the primary building blocks of convolutional neural networks (CNNs), which are used in many machine learning tasks.\n",
    "- In general, CNNs work well on tasks where the data/input features have some level of partial relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2    Dataset <a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Fashion-MNIST__ is a dataset of Zalando's article images - consisting of a training set of 60,000 examples and a test set of 10,000 examples | Each example is a $28x28$ grayscale image, associated with 10 classes |  Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms | It shares the same image size and structure of training and testing splits | \n",
    "\n",
    "- Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "    - To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\n",
    "    - For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "\n",
    "- __Labels__: Each training and test example is assigned to one of the following labels:\n",
    "<center>\n",
    "\n",
    "0 | T-shirt/top\n",
    "--|------------\n",
    "1 | Trouser\n",
    "2 | Pullover\n",
    "3 | Dress\n",
    "4 | Coat\n",
    "5 | Sandal\n",
    "6 | Shirt\n",
    "7 | Sneaker\n",
    "8 | Bag\n",
    "9 | Ankle boot\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3    Motivation <a id=\"motivation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelizing the convolutional layer in the LeNet-5 architecture using CUDA offers several benefits:\n",
    "\n",
    "1. **Speedup**: The convolutional layer is the most computationally intensive layer in the LeNet-5 architecture. By parallelizing this layer, we can significantly reduce the training time of the network, leading to faster model convergence and improved overall performance.\n",
    "\n",
    "2. **Scalability**: The parallelized convolutional layer can be seamlessly integrated into other convolutional neural networks, such as AlexNet, VGGNet, GoogLeNet, ResNet, YOLO, and many more. This means that the optimizations made for the LeNet-5 architecture can be applied to a wide range of deep learning models, enhancing their efficiency and scalability.\n",
    "\n",
    "By leveraging the power of CUDA and parallel computing, we can unlock the full potential of convolutional neural networks, enabling faster and more efficient training of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background <a id=\"background\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1    Convolutional Neural Networks (CNNs)<a id=\"cnns\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standford [cheatsheat](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#overview)\n",
    "  \n",
    "- Video CNNs: [\"How Convolutional Neural Networks work\"](https://www.youtube.com/watch?v=FmpDIaiMIeA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2    LeNet-5 <a id=\"lenet-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg)\n",
    "\n",
    "</center>\n",
    "\n",
    "- A Complete Guide: [here](https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Starter project <a id=\"starter-project\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the open source [mini-dnn-cpp](https://github.com/iamhankai/mini-dnn-cpp) as the starter project. It is well-designed for training convolutional neural networks on CPUs. The project is written in C++ and uses the [Eigen 3.3.4](http://bitbucket.org/eigen/eigen/get/3.3.4.tar.bz2) library for matrix operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementation <a id=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original starter project was used the Eigen library version 3.3.4, which is not compatible with CUDA. Therefore, we have to upgrade the Eigen library to version 3.4.0, which is compatible with CUDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    ".\n",
    "├── build/                                  Build directory\n",
    "├── checkpoint/                             Checkpoint directory\n",
    "├── data/                                   Data directory\n",
    "│   ├── fashion-mnist\n",
    "│   └── mnist\n",
    "├── Eigen/                                  Eigen library\n",
    "├── src/                                    Source directory\n",
    "│   ├── layer/                              Layer directory\n",
    "│   │     ├── gpu/                          GPU directory\n",
    "│   │     │   ├── conv_cpu.cc               Convolutional layer CPU implementation\n",
    "│   │     │   ├── conv_kernel1.cu           Convolutional layer GPU optimized v1 implementation\n",
    "│   │     │   ├── conv_kernel2.cu           Convolutional layer GPU optimized v2 implementation\n",
    "│   │     │   ├── conv_kernel3.cu           Convolutional layer GPU optimized v3 implementation\n",
    "│   │     │   ├── conv_kernel.cu            Convolutional layer GPU basic implementation\n",
    "│   │     │   ├── gpu_interface.h           GPU interface\n",
    "│   │     │   ├── utils.cu                  \n",
    "│   │     │   └── utils.h                   GPU utils\n",
    "│   │     ├── ave_pooling.cc                \n",
    "│   │     ├── ave_pooling.h                 Average pooling layer CPU implementation\n",
    "│   │     ├── conv.cc\n",
    "│   │     ├── conv.h                        Convolutional layer CPU \n",
    "│   │     ├── conv_gpu.cc\n",
    "│   │     ├── conv_gpu.h                    Convolutional layer GPU \n",
    "│   │     ├── fully_connected.cc\n",
    "│   │     ├── fully_connected.h             Fully connected layer CPU implementation\n",
    "│   │     ├── max_pooling.cc\n",
    "│   │     ├── max_pooling.h                 Max pooling layer CPU implementation\n",
    "│   │     ├── relu.cc\n",
    "│   │     ├── relu.h                        ReLU activation function CPU implementation\n",
    "│   │     ├── sigmoid.cc\n",
    "│   │     ├── sigmoid.h                     Sigmoid activation function CPU implementation\n",
    "│   │     ├── softmax.cc\n",
    "│   │     └── softmax.h                     Softmax activation function CPU implementation\n",
    "│   ├── loss/                               Loss directory\n",
    "│   ├── optimizer/                          Optimizer directory\n",
    "│   ├── CMakeLists.txt\n",
    "│   ├── layer.h                             Layer interface\n",
    "│   ├── loss.h                              Loss interface\n",
    "│   ├── mnist.cc                            \n",
    "│   ├── mnist.h                             MNIST/Fashion-MNIST dataset loader\n",
    "│   ├── network.cc          \n",
    "│   ├── network.h                           Network interface   \n",
    "│   ├── optimizer.h                         Optimizer interface\n",
    "│   └── utils.h                             Utils: accuracy, one hot encoding, ...\n",
    "├── third_party/\n",
    "├── CMakeLists.txt\n",
    "├── LICENSE\n",
    "├── Makefile\n",
    "├── readme.md\n",
    "├── report.ipynb\n",
    "├── test.cc                                 Testing script\n",
    "├── train.cc                                Training script\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1    Convolutional layer<a id=\"convolutional-layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution layer (CONV) uses filters that perform convolution operations as it is scanning the input $\\mathbb{I}$ with respect to its dimensions. Its hyperparameters include the filter size $\\mathbb{F}$ and stride $\\mathbb{S}$. The resulting output $\\mathbb{O}$ is called feature map or activation map.\n",
    "<center>\n",
    "\n",
    "![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-cnn-en.jpeg?3b7fccd728e29dc619e1bd8022bf71cf)\n",
    "\n",
    "</center> \n",
    "\n",
    "The sequential implementation of the convolutional layer was use the im2col method to convert the input image into a matrix and then perform matrix multiplication with the filter matrix to get the output image. The im2col algorithm is a technique that converts an image into a matrix, such that each column of the matrix corresponds to a small patch of the image. This makes it easier to perform convolution operations using matrix multiplication, which can be faster and more efficient than looping over the image pixels. The im2col method is working as follows:\n",
    "- Convert input image of size O(HWC) to a patches matrix of size O(HW(K^2)C) \n",
    "- Convert filter into format __kernel height__ * __kernel width__ * __kernel channel__\n",
    "- Multiply the modified input and filter matrix using GEMM matrix multiplication to get the output. This is a single call.\n",
    "\n",
    "To illustrate the im2col algorithm with a greyscale example, let's assume we have a 4x4 image with one channel, and we want to apply a 2x2 filter with a stride of 1 and no padding. The im2col algorithm would produce a 4x9 matrix, where each column represents a 2x2 patch of the image, as shown below:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & c & d \\\\\n",
    "e & f & g & h \\\\\n",
    "i & j & k & l \\\\\n",
    "m & n & o & p \\\\\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "a & b & c & e & f & g & i & j & k \\\\\n",
    "b & c & d & f & g & h & j & k & l \\\\\n",
    "e & f & g & i & j & k & m & n & o \\\\\n",
    "f & g & h & j & k & l & n & o & p \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we can multiply this matrix by a vectorized version of the filter, and reshape the result into a 3x3 output image. For example, if the filter is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the vectorized filter is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the output image is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a + 2b + 3e + 4f & b + 2c + 3f + 4g & c + 2d + 3g + 4h \\\\\n",
    "e + 2f + 3i + 4j & f + 2g + 3j + 4k & g + 2h + 3k + 4l \\\\\n",
    "i + 2j + 3m + 4n & j + 2k + 3n + 4o & k + 2l + 3o + 4p \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is equivalent to sliding the filter over the image and computing the dot product at each position, but it can be done more efficiently using matrix operations. \n",
    "\n",
    "Here is the pseudo code for the im2col method:\n",
    "\n",
    "```c++\n",
    "input[C][H][W];\n",
    "kernels[M][K][K][C];\n",
    "output[M][H][W];\n",
    "for h in 1 to H do\n",
    "    for w in 1 to W do\n",
    "        for o in 1 to M do\n",
    "            sum = 0;\n",
    "            for x in 1 to K do\n",
    "                for y in 1 to K do\n",
    "                    for i in 1 to C do\n",
    "                    sum += input[i][h+y][w+x] * kernels[o][x][y][i];\n",
    "         output[o][w][h] = sum;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2    Pooling layer<a id=\"pooling-layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooling layer is a downsampling operation, typically applied after a convolution layer, which does some spatial invariance. There has been a lot of research on the pooling layer, and the most common pooling methods are max pooling and average pooling. In this project, we will implement both of them. \n",
    "<p align=\"center\">\n",
    "    <img src=\"https://www.mdpi.com/remotesensing/remotesensing-13-04712/article_deploy/html/images/remotesensing-13-04712-g005.png\" width=\"600\" height=\"400\">\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "\n",
    "Type | Max pooling | Average pooling\n",
    "-----|-------------|----------------\n",
    "Purpose | Each pooling operation selects the maximum value of the current view | Each pooling operation averages the values of the current view\n",
    "Illustration | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/max-pooling-a.png?711b14799d07f9306864695e2713ae07) | ![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/average-pooling-a.png?58f9ab6d61248c3ec8d526ef65763d2f)\n",
    "Comments | - Preserves detected features <br> - Most commonly used | - Downsamples features map <br> - Used in LeNet\n",
    "\n",
    "</center>\n",
    "\n",
    "In our implementation, max pooling was utilized. Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max pooling will be a feature map containing the most prominent features of the previous feature map. \n",
    "\n",
    "Because the pooling layer dont have any trainable parameters, the backward pass is very simple. We just need to propagate the gradient from the output feature map to the input feature map. \n",
    "\n",
    "\n",
    "In particular, max and average pooling are special kinds of pooling where the maximum and average value is taken, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3    Fully connected layer<a id=\"fully-connected-layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully connected layer (FC) operates on a flatten input where each input is connected to all neurons. It also called a dense layer, and it is the most common layer in neural networks. The fully connected layer is implemented as a matrix multiplication between the input $\\mathbb{I}$ and the weights $\\mathbb{W}$, followed by the addition of the bias $\\mathbb{B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4    LeNet-5 architecture<a id=\"lenet-5-architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the original LeNet-5 architecture, we will use a modified version of it. The original LeNet-5 architecture is shown below:\n",
    "![](https://d2l.ai/_images/lenet.svg)\n",
    "And the modified version is shown below:\n",
    "\n",
    "__First Layer: Convolutional Layer (CONV1):__\n",
    "\n",
    "- Parameters: Input (N) = 28, Padding (P) = 2, Filter (F) = 5 x 5, Stride (S) = 1\n",
    "- Conv Operation: ((N + 2P - F) / S) + 1 = ((28 + 4 - 5) / 1) + 1 = 28 x 28\n",
    "- We will apply 6 filters / kernels so we will get a 28 x 28 x 6 dimensional output\n",
    "\n",
    "__Second Layer: Max Pooling Layer (POOL1):__\n",
    "\n",
    "- Parameters: Input (N) = 28, Filter (F) = 2 x 2, Stride (S) = 2\n",
    "- AVG Pooling Operation: ((N + 2P -F) / S) + 1 = ((28 - 2) / 2) + 1 = 14 x 14\n",
    "- We will have a 14 x 14 x 6 dimensional output at the end of this pooling\n",
    "\n",
    "__Third Layer: Convolutional Layer (CONV2):__\n",
    "\n",
    "- Parameters: Input (N) = 14, Filter (F) = 5 x 5, Stride (S) = 1\n",
    "- Conv Operation: ((N + 2P - F) / S) + 1 = ((14 - 5) / 1) + 1 = 10 x 10\n",
    "- We will apply 16 filters / kernels so we will get a 10 x 10 x 16 dimensional output\n",
    "\n",
    "__Fourth Layer: Max Pooling Layer (POOL2):__\n",
    "\n",
    "- Parameters: Input (N) = 10, Filter (F) = 2 x 2, Stride (S) = 2\n",
    "- AVG Pooling Operation: ((N + 2P -F) / S) + 1 = ((10 - 2) / 2) + 1 = 5 x 5\n",
    "- We will have a 5 x 5 x 16 dimensional output at the end of this pooling\n",
    "\n",
    "__Fifth Layer: Fully Connected layer(FC1):__\n",
    "\n",
    "- Parameters: W: 400 * 120, b: 120\n",
    "- We will have an output of 120 x 1 dimension\n",
    "\n",
    "__Sixth Layer: Fully Connected layer(FC2):__\n",
    "\n",
    "- Parameters: W: 120 * 84, b: 84\n",
    "- We will have an output of 84 x 1 dimension\n",
    "\n",
    "__Seventh Layer: Output layer(Softmax):__\n",
    "\n",
    "- Parameters: W: 84 * 10, b: 10\n",
    "- We will get an output of 10 x 1 dimension\n",
    "\n",
    "```python\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5    Parallelization <a id=\"parallelization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the teacher's instructions, we will only parallelize the __forward-pass__ of the convolutional layer using CUDA. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1    Basic kernel<a id=\"basickernel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize the convolutional layer, we will use CUDA to implement a kernel function that performs the convolution operation in parallel. The kernel function will be called by the CPU and executed by the GPU. Fisrt, It will calculate the index of the output feature map, input feature map, and filter. Then, it loop over each channel in the input feature map and each element in the filter to calculate the convolution operation. Finally, it will store the result in the output feature map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2    Optimized kernel 1<a id=\"kernel1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, we will use shared memory to store the input feature map. This will reduce the number of global memory accesses, which is the main bottleneck of the convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3    Optimized kernel 2<a id=\"kernel2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, we add an extra shared memory to do convolution.\n",
    "\n",
    "- Declare a shared memory variable that is used to store a tile of the input data. The size of the tile is determined by the number of threads in a block and the size of the shared memory.\n",
    "\n",
    "- Copy the input data from global memory to shared memory. This is done by each thread in a block copying a portion of the input data to the shared memory variable.\n",
    "\n",
    "- Compute the dot product of the shared memory variable and the convolutional filters, which are stored in constant memory. This is done by each thread in a block computing the dot product of the portion of the input data stored in shared memory and the corresponding portion of the convolutional filters stored in constant memory.\n",
    "\n",
    "- Sum the dot products computed by all threads in a block to obtain the output feature maps for the block.\n",
    "Write the output feature maps from shared memory to global memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4    Optimized kernel 3<a id=\"kernel3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, I will use the combination of shared memory and constant memory to store the input feature map and the filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6    Save and load model<a id=\"save-load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the model, we need to save and load the weights of the model. We implemented the `save_parameters` and `load_parameters` methods in the `Network` class to save and load the weights of the model with binary files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Usage <a id=\"usage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Prerequisites <a id=\"prerequisites\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download and unzip [FASHION-MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist) dataset in `mini-dnn-cpp/data/fashion-mnist/`.\n",
    "\n",
    "- Download and unzip [Eigen 3.4.0](https://gitlab.com/libeigen/eigen/-/releases/3.4.0), then place folder __Eigen__ in `mini-dnn-cpp/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Training <a id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !make setup\n",
    "# !make train\n",
    "# !make train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Testing <a id=\"testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following command after each test to clean up the temporary files:\n",
    "\n",
    "```bash\n",
    "make clean\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 CPU <a id=\"cpu\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make your changes take effect please reactivate your environment\n",
      "rm -f src/layer/*.o\n",
      "rm test.o\n",
      "rm test\n",
      "rm: cannot remove 'test': No such file or directory\n",
      "make: *** [Makefile:86: clean] Error 1\n",
      "To make your changes take effect please reactivate your environment\n",
      "make network.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make mnist.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make layer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make loss\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make optimizer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/layer/gpu/utils.cu -o src/layer/utils.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "nvcc --compile src/layer/gpu/conv_cpu.cc -o src/layer/conv_cpu.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile test.cc -o test.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "./test\n",
      "**********GPU info**********\n",
      "Name: NVIDIA RTX A6000\n",
      "Compute capability: 8.6\n",
      "Num SMs: 84\n",
      "Max num threads per SM: 1536\n",
      "Max num warps per SM: 48\n",
      "GMEM: 51050250240 byte\n",
      "SMEM per SM: 102400 byte\n",
      "SMEM per block: 49152 byte\n",
      "****************************\n",
      "mnist train number: 60000\n",
      "mnist test number: 10000\n",
      "Num Layers: 12\n",
      "Convolution c1 - GPU\t - Layer Time: 8500.75 ms\n",
      "Convolution c3 - GPU\t - Layer Time: 14941.2 ms\n",
      "\n",
      "Test accuraccy on GPU: 0.8619\n"
     ]
    }
   ],
   "source": [
    "!make clean\n",
    "!make setup\n",
    "!make cpu\n",
    "!make test\n",
    "!make run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Basic kernel <a id=\"basic-kernel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make your changes take effect please reactivate your environment\n",
      "rm -f src/layer/*.o\n",
      "rm test.o\n",
      "rm test\n",
      "rm: cannot remove 'test': No such file or directory\n",
      "make: *** [Makefile:76: clean] Error 1\n",
      "To make your changes take effect please reactivate your environment\n",
      "make network.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make mnist.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make layer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make loss\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make optimizer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/layer/gpu/utils.cu -o src/layer/utils.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "nvcc --compile src/layer/gpu/conv_kernel.cu -o src/layer/conv_kernel.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile test.cc -o test.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "./test\n",
      "**********GPU info**********\n",
      "Name: NVIDIA RTX A6000\n",
      "Compute capability: 8.6\n",
      "Num SMs: 84\n",
      "Max num threads per SM: 1536\n",
      "Max num warps per SM: 48\n",
      "GMEM: 51050250240 byte\n",
      "SMEM per SM: 102400 byte\n",
      "SMEM per block: 49152 byte\n",
      "****************************\n",
      "mnist train number: 60000\n",
      "mnist test number: 10000\n",
      "Num Layers: 12\n",
      "Convolution c1 - GPU. Not Optimize:\n",
      "\t - Kernel Time: 1.37011 ms\n",
      "\t - Layer Time: 72.7324 ms\n",
      "Convolution c3 - GPU. Not Optimize:\n",
      "\t - Kernel Time: 3.57376 ms\n",
      "\t - Layer Time: 33.3943 ms\n",
      "\n",
      "Test accuraccy on GPU: 0.8619\n"
     ]
    }
   ],
   "source": [
    "!make clean\n",
    "!make setup\n",
    "!make gpu_basic\n",
    "!make test\n",
    "!make run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Optimized kernel 1 <a id=\"optimize-kernel-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make your changes take effect please reactivate your environment\n",
      "rm -f src/layer/*.o\n",
      "rm test.o\n",
      "rm test\n",
      "rm: cannot remove 'test': No such file or directory\n",
      "make: *** [Makefile:76: clean] Error 1\n",
      "To make your changes take effect please reactivate your environment\n",
      "make network.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make mnist.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make layer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make loss\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make optimizer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/layer/gpu/utils.cu -o src/layer/utils.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "nvcc --compile src/layer/gpu/conv_kernel1.cu -o src/layer/conv_kernel1.o -I./ -L/usr/local/cuda/lib64 -lcudart  \n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile test.cc -o test.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "./test\n",
      "**********GPU info**********\n",
      "Name: NVIDIA RTX A6000\n",
      "Compute capability: 8.6\n",
      "Num SMs: 84\n",
      "Max num threads per SM: 1536\n",
      "Max num warps per SM: 48\n",
      "GMEM: 51050250240 byte\n",
      "SMEM per SM: 102400 byte\n",
      "SMEM per block: 49152 byte\n",
      "****************************\n",
      "mnist train number: 60000\n",
      "mnist test number: 10000\n",
      "Num Layers: 12\n",
      "Convolution c1 - GPU. Optimize ver 1:\n",
      "\t - Kernel Time: 1.77152 ms\n",
      "\t - Layer Time: 74.2185 ms\n",
      "Convolution c3 - GPU. Optimize ver 1:\n",
      "\t - Kernel Time: 4.41344 ms\n",
      "\t - Layer Time: 33.8345 ms\n",
      "\n",
      "Test accuraccy on GPU: 0.8619\n"
     ]
    }
   ],
   "source": [
    "!make clean\n",
    "!make setup\n",
    "!make gpu_v1\n",
    "!make test\n",
    "!make run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the optimized kernel 1 is slower than the basic kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.4 Optimized kernel 2 <a id=\"optimized-kernel-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make your changes take effect please reactivate your environment\n",
      "rm -f src/layer/*.o\n",
      "rm test.o\n",
      "rm test\n",
      "To make your changes take effect please reactivate your environment\n",
      "make network.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make mnist.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make layer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make loss\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make optimizer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/layer/gpu/utils.cu -o src/layer/utils.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "nvcc --compile src/layer/gpu/conv_kernel2.cu -o src/layer/conv_kernel2.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile test.cc -o test.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "./test\n",
      "**********GPU info**********\n",
      "Name: NVIDIA RTX A6000\n",
      "Compute capability: 8.6\n",
      "Num SMs: 84\n",
      "Max num threads per SM: 1536\n",
      "Max num warps per SM: 48\n",
      "GMEM: 51050250240 byte\n",
      "SMEM per SM: 102400 byte\n",
      "SMEM per block: 49152 byte\n",
      "****************************\n",
      "mnist train number: 60000\n",
      "mnist test number: 10000\n",
      "Num Layers: 12\n",
      "Convolution c1 - GPU. Optimize ver 2: Constant-shared memory\n",
      "\t - Kernel Time: 1.39878 ms\n",
      "\t - Layer Time: 76.6955 ms\n",
      "Convolution c3 - GPU. Optimize ver 2: Constant-shared memory\n",
      "\t - Kernel Time: 3.3065 ms\n",
      "\t - Layer Time: 33.7417 ms\n",
      "\n",
      "Test accuraccy on GPU: 0.8619\n"
     ]
    }
   ],
   "source": [
    "!make clean\n",
    "!make setup\n",
    "!make gpu_v2\n",
    "!make test\n",
    "!make run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimze kernel 2 have slightly speedup compared to the basic kernel for the C3 layer. However, it is slower than the basic kernel for the C1 layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.5 Optimized kernel 3 <a id=\"optimized-kernel-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "major, minor = cuda.get_current_device().compute_capability\n",
    "print(f'GPU compute capability: {major}.{minor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the GPU compute capability of 8.6 dont support the half-precision floating-point format, so I use a compute capability of 7.5 to test the optimized kernel 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make your changes take effect please reactivate your environment\n",
      "rm -f src/layer/*.o\n",
      "rm test.o\n",
      "rm test\n",
      "To make your changes take effect please reactivate your environment\n",
      "make network.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make mnist.o\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make layer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make loss\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "make optimizer\n",
      "make[1]: Entering directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "make[1]: Leaving directory '/mnt/net/i2x256-ai03/hotel/phit/personal/ParallelProgramming/mini-dnn-cpp'\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/layer/gpu/utils.cu -o src/layer/utils.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "nvcc -arch=sm_75 --compile src/layer/gpu/conv_kernel3.cu -o src/layer/conv_kernel3.o -I./ -L/usr/local/cuda/lib64 -lcudart \n",
      "\u001b[01m\u001b[0m\u001b[01msrc/layer/gpu/conv_kernel3.cu(62)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity loc_mem1 is treated as a static definition\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01msrc/layer/gpu/conv_kernel3.cu(130)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity loc_mem2 is treated as a static definition\n",
      "\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile test.cc -o test.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "To make your changes take effect please reactivate your environment\n",
      "nvcc --compile src/network.cc -o src/network.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/mnist.cc -o src/mnist.o  -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/ave_pooling.cc -o src/layer/ave_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv.cc -o src/layer/conv.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/conv_gpu.cc -o src/layer/conv_gpu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/fully_connected.cc -o src/layer/fully_connected.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/max_pooling.cc -o src/layer/max_pooling.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/relu.cc -o src/layer/relu.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/sigmoid.cc -o src/layer/sigmoid.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/layer/softmax.cc -o src/layer/softmax.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/cross_entropy_loss.cc -o src/loss/cross_entropy_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/loss/mse_loss.cc -o src/loss/mse_loss.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc --compile src/optimizer/sgd.cc -o src/optimizer/sgd.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "nvcc -o test -lm -lcuda -lrt test.o src/network.o src/mnist.o src/layer/*.o src/loss/*.o src/optimizer/*.o -I./ -L/usr/local/cuda/lib64 -lcudart\n",
      "./test\n",
      "**********GPU info**********\n",
      "Name: NVIDIA RTX A6000\n",
      "Compute capability: 8.6\n",
      "Num SMs: 84\n",
      "Max num threads per SM: 1536\n",
      "Max num warps per SM: 48\n",
      "GMEM: 51050250240 byte\n",
      "SMEM per SM: 102400 byte\n",
      "SMEM per block: 49152 byte\n",
      "****************************\n",
      "mnist train number: 60000\n",
      "mnist test number: 10000\n",
      "Num Layers: 12\n",
      "Convolution c1 - GPU. Optimize ver 3:\n",
      "\t - Kernel Time: 1.17472 ms\n",
      "\t - Layer Time: 197.065 ms\n",
      "Convolution c3 - GPU. Optimize ver 3:\n",
      "\t - Kernel Time: 1.1623 ms\n",
      "\t - Layer Time: 91.6598 ms\n",
      "\n",
      "Test accuraccy on GPU: 0.862\n"
     ]
    }
   ],
   "source": [
    "!make clean\n",
    "!make setup\n",
    "!make gpu_v3\n",
    "!make test\n",
    "!make run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above findings, we can see that the optimized kernel 3 is faster in the Conv c3 layer. However, its total layer time is slower than the other kernels. This is because the optimized kernel 3 uses a faster type of memory called shared memory, but it can only store a small amount of data. So, we have to divide the data into smaller parts and copy them to shared memory, which takes more time and slows down the overall performance.\n",
    "\n",
    "- On the other hand, the optimized kernel 3 has a slightly higher accuracy of 0.862 compared to the original accuracy. This could be because the way it calculates numbers may cause some rounding errors, leading to slightly different results. Normally, using a less precise type of number format like half-precision floating-point would result in lower accuracy, but in this case, it gave slightly better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results <a id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Training Stage <a id=\"training-stage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "  Network dnn;\n",
    "  Layer *conv1 = new Conv(1, 28, 28, 6, 5, 5);\n",
    "  Layer *pool1 = new MaxPooling(6, 24, 24, 2, 2, 2);\n",
    "  Layer *conv2 = new Conv(6, 12, 12, 16, 5, 5);\n",
    "  Layer *pool2 = new MaxPooling(16, 8, 8, 2, 2, 2);\n",
    "  Layer* fc3 = new FullyConnected(pool2->output_dim(), 120);\n",
    "  Layer* fc4 = new FullyConnected(120, 84);\n",
    "  Layer* fc5 = new FullyConnected(84, 10);\n",
    "\n",
    "  Layer* relu1 = new ReLU;\n",
    "  Layer* relu2 = new ReLU;\n",
    "  Layer* relu3 = new ReLU;\n",
    "  Layer* relu4 = new ReLU;\n",
    "  Layer* relu5 = new ReLU;\n",
    "  Layer* softmax = new Softmax;\n",
    "  dnn.add_layer(conv1);\n",
    "  dnn.add_layer(relu1);\n",
    "  dnn.add_layer(pool1);\n",
    "  dnn.add_layer(conv2);\n",
    "  dnn.add_layer(relu2);\n",
    "  dnn.add_layer(pool2);\n",
    "  dnn.add_layer(fc3);\n",
    "  dnn.add_layer(relu3);\n",
    "  dnn.add_layer(fc4);\n",
    "  dnn.add_layer(relu4);\n",
    "  dnn.add_layer(fc5);\n",
    "  dnn.add_layer(softmax);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default hyperparameters:\n",
    "\n",
    "```c++\n",
    "  int batch_size = 128;\n",
    "  float learning_rate = 0.001;\n",
    "```\n",
    "\n",
    "1. Train on Fashion-MNIST with 10 epochs\n",
    "```\n",
    "weight-bad-1.bin\n",
    "10-th epoch, test acc: 0.6136\n",
    "```\n",
    "\n",
    "2. Train on Fashion-MNIST with 10 epochs (dont use Relu activation at the (n - 1) layer (before softmax layer)). ([We found out the use of ReLU activation function at the penultimate layer is not good for the model, so we decided to remove it](https://stats.stackexchange.com/questions/163695/non-linearity-before-final-softmax-layer-in-a-convolutional-neural-network)) \n",
    "```\n",
    "weight-1.bin\n",
    "10-th epoch, test acc: 0.8619\n",
    "```\n",
    "\n",
    "<!-- \n",
    "3. Train on Fashion-MNIST with 10 epochs with kernel size = 2\n",
    "```\n",
    "weight-bad-2.bin\n",
    "10-th epoch, test acc: 0.625\n",
    "```\n",
    "\n",
    "4. Train on Fashion-MNIST with 10 epochs with kernel size = 2 and dont use Relu at the penultimate layer\n",
    "```\n",
    "weight-2.bin\n",
    "10-th epoch, test acc: 0.8766\n",
    "```  \n",
    "\n",
    "5. Train on Fashion-MNIST with 15 epochs with kernel size = 2 and dont use Relu at the penultimate layer\n",
    "```\n",
    "weight-3.bin\n",
    "15-th epoch, test acc: 0.8862\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Testing Stage <a id=\"testing-stage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the testing stage, we will only use the `weight-1.bin` checkpoint to optimize the convolutional layer using CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel | Convolution layer | Kernel time | Layer time | Accuracy \n",
    "-------|-------------------|-------------|------------|---------\n",
    "CPU | (C1) |            | 8500.75 ms | 0.8619\n",
    "Basic kernel | (C1) | 1.37126 ms | 73.3622 ms | 0.8619\n",
    "Kernel 1 | (C1) | 1.77152 ms | 74.2185 ms | 0.8619\n",
    "Kernel 2 | (C1) | 1.39878 ms | 76.6955 ms | 0.8619\n",
    "Kernel 3 | (C1) | 1.17472 ms | 197.065 ms | 0.862\n",
    "\n",
    "Kernel | Convolution layer | Kernel time | Layer time | Accuracy \n",
    "-------|-------------------|-------------|------------|---------\n",
    "CPU    | (C3) |            | 14941.2 ms | 0.8619\n",
    "Basic kernel | (C3) | 3.58912 ms | 34.251 ms | 0.8619\n",
    "Kernel 1 | (C3) | 4.92134 ms | 33.8345 ms | 0.8619\n",
    "Kernel 2 | (C3) | 3.3065 ms | 33.7417 ms | 0.8619\n",
    "Kernel 3 | (C3) | 1.1623 ms | 91.6598 ms | 0.862\n",
    "\n",
    "We can see that the layer time is the same for all kernels, but the kernel time is different. This is because the kernel time is the time it takes to execute the kernel function on the GPU, while the layer time is the time it takes to execute the entire layer, including the kernel function and the data transfer between the CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion <a id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we have explored various aspects of training and testing a deep neural network using the Fashion-MNIST dataset. We started by defining the architecture of the network, which consisted of convolutional layers, pooling layers, and fully connected layers. We implemented different kernel functions to parallelize the convolutional layer using CUDA, and compared their performance in terms of kernel time and layer time.\n",
    "\n",
    "During the training stage, we experimented with different hyperparameters and observed the impact on the accuracy of the model. We also tested different variations of the network architecture, such as removing the ReLU activation function at the penultimate layer. We saved and loaded the weights of the trained model for testing purposes.\n",
    "\n",
    "In the testing stage, we compared the performance of the different kernel functions in terms of kernel time, layer time, and accuracy. We observed that the optimized kernel 3, which was combined of several optimization techniques, had the fastest kernel time but a slower layer time due to the data transfer overhead. We also applied mutiple techniques to improve the performance of the convolutional layer, such as using shared memory, constant memory, unrolling loops, tile-based matrix multiplication, and half-precision floating-point format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Reflection <a id=\"reflection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main challenge of this project was to understand the CUDA programming techniques, the way to interact between CPU and GPU, and the way to parallelize the convolutional layer. We had to read a lot of documentation and tutorials to get familiar with the codebase and the CUDA programming model. Throughout the project, we have expand our knowledge of CUDA programming and parallel computing, which will be very useful in our future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. References <a id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fashion-MNIST Dataset: https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
    "- Starter project (mini-dnn-cpp): https://github.com/iamhankai/mini-dnn-cpp\n",
    "- Eigen library (3.4.0): https://gitlab.com/libeigen/eigen/-/releases/3.4.0\n",
    "- Course slides, demos, and project description (CSC14120 – PARALLEL PROGRAMMING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zalo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
